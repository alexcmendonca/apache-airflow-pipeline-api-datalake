# Desenvolvimento de um Pipeline de Dados em Python para a extra√ß√£o de dados por meio de uma API, utilizando o Apache Airflow.

## üí°Objetivos
Este projeto tem como objetivo a cria√ß√£o de um pipeline de dados conectado √†s redes sociais, com foco especial no Twitter. A proposta √© extrair tweets que abordem t√≥picos espec√≠ficos da nossa √°rea de interesse.
O intuito √© compreender as rea√ß√µes das pessoas em rela√ß√£o a determinados termos e analisar os coment√°rios nas redes sociais. Para isso, realizaremos a extra√ß√£o e armazenamento dos tweets, possibilitando o acesso aos dados para an√°lises, modelagem posterior, e mantendo todo esse processo de maneira automatizada.
A automa√ß√£o do pipeline foi implementada utilizando o Apache Airflow, um orquestrador de tarefas amplamente reconhecido e utilizado no mercado atual. O Airflow desempenha o papel fundamental de executar tarefas programadas, desde a extra√ß√£o de dados da API (com um comportamento semelhante ao da API do Twitter) at√© a disponibiliza√ß√£o desses dados em um arquivo.
Cabe ressaltar que a escolha de replicar o comportamento da API do Twitter se deve ao fato de que a API original tornou-se paga, dificultando seu uso como fonte de dados para fins de treinamento.


## üñ•Ô∏èDesafios do Projeto
Neste projeto de Engenharia de Dados, enfrentamos o desafio de estabelecer uma conex√£o com uma API por meio do Python, com o objetivo de extrair dados usando o Apache Airflow, um orquestrador de fluxo de dados. Com essa ferramenta, √© poss√≠vel planejar as tarefas de extra√ß√£o e armazenamento de dados em um Data Lake.

No Apache Airflow, foram configurados o Hook, o Operator e os DAGs, conferindo ao Airflow a capacidade de realizar uma orquestra√ß√£o eficiente.

**- Hook:** Respons√°vel pelo gerenciamento das conex√µes, determina quando abrir e encerrar sess√µes. A vantagem de sua utiliza√ß√£o est√° na capacidade de compartilhar essas sess√µes com outras opera√ß√µes, gerenciando de forma abrangente, considerando que n√£o apenas nossa DAG pode estar realizando uma conex√£o e que pode haver um limite de conex√µes.

**- Operator:** Encapsula a l√≥gica para realizar uma unidade de trabalho (task). Define as a√ß√µes a serem conclu√≠das para cada unidade de trabalho e abstrai muito c√≥digo que seria necess√°rio escrever. O Airflow possui um amplo conjunto de operadores dispon√≠veis. Contudo, se n√£o houver um operador para um caso de uso espec√≠fico, √© poss√≠vel criar operadores personalizados, desenvolvendo classes em Python. Um operador possui tr√™s caracter√≠sticas principais:

        1. Idempot√™ncia: Independentemente de quantas vezes uma tarefa for executada com os mesmos par√¢metros, o resultado final deve ser sempre o mesmo;
        2. Isolamento: A tarefa n√£o compartilha recursos com outras tarefas de qualquer outro operador;
        3. Atomicidade: A tarefa √© um processo indivis√≠vel e bem determinado.

**- DAG (Directed Acyclic Graphs):** √â respons√°vel por automatizar o fluxo de dados em nosso pipeline, dividindo um trabalho extenso em uma ou mais etapas, representadas por "tarefas" individuais (tasks) que, quando combinadas, formam um DAG.

###### Imagem 1: Diagrama do Pipeline deste Projeto (Cr√©ditos da imagem: Alura)
<img src="/assets/img/img_projeto02_diagrama_pipeline.png">

## üìÑDesenvolvimento de Compet√™ncias:
|Atividades|Realizadas |
|----------|-----------|
| Reconhecer o que √© o Airflow | Identificar um processo de Data Pipeline |
| Diferen√ßas entre ETL e ELT | Elaborar uma requisi√ß√£o para API do Twitter |
| Extrair as informa√ß√µes de json | Utilizar a mec√¢nica de resultados por p√°ginas do Twitter |
| Criar e utilizar um ambiente virtual | Instalar o Airflow |
| Identificar o que √© um Hook | Prototipar um Hook |
| Implementar conex√µes Http no Airflow | Reconhecer o que √© um Operator |
| Elaborar um Operator | Identificar o que √© um DataLake |
| Reconhecer o que √© um DAG | Prototipar uma DAG |

## üóÇÔ∏èOrganiza√ß√£o dos Arquivos
- SRC: Cont√©m o script em Python respons√°vel pela extra√ß√£o dos dados da API.
- Hook: Implementa√ß√£o de um Hook no Airflow para realizar a conex√£o com a API utilizando o HTTP Hook. Esse Hook √© utilizado para efetuar as requisi√ß√µes, utilizando a configura√ß√£o de uma conex√£o no Airflow.
- Operators: Desenvolvimento de um operador para extrair dados da API, criando um Data Lake e armazenando os dados nesse reposit√≥rio.
- DAGs: Automa√ß√£o do pipeline realizada por meio do DAG, no qual s√£o especificadas as instru√ß√µes para o Airflow sobre como a extra√ß√£o do operador ser√° executada. Define-se par√¢metros como frequ√™ncia, data inicial e data final.

## üéûÔ∏èImagens do Projeto

###### Imagem 2: Interface Airflow - Lista DAGs
<img src="/assets/img/img_projeto02_dags.png">

###### Imagem 3: Visualiza√ß√£o das "connections" e sua aplica√ß√£o com a cria√ß√£o de Hooks
<img src="/assets/img/img_projeto02_connection.png">


## üîçRefer√™ncias
- [Alura](https://www.alura.com.br/)
